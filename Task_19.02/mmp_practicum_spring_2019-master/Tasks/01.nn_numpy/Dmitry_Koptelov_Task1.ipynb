{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 практическое задание. Обучение полносвязной нейронной сети.\n",
    "\n",
    "## Практикум на ЭВМ для 317 группы, весна 2019\n",
    "\n",
    "#### Фамилия, имя: Коптелов Дмитрий\n",
    "\n",
    "Дата выдачи: 19 февраля\n",
    "\n",
    "Мягкий дедлайн: 28 февраля 23:59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация нейронной сети (6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой нелинейности ReLU\n",
    "\n",
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        layer = np.copy(input)\n",
    "        layer[layer < 0] = 0\n",
    "        self.layer = layer\n",
    "        return layer        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        d_layer_d_input = np.copy(self.layer)\n",
    "        d_layer_d_input[d_layer_d_input > 0] = 1\n",
    "        return grad_output * d_layer_d_input, []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54cf5cfc9ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        layer = np.dot(input, self.weights) + self.biases\n",
    "        self.input = input\n",
    "        self.layer = layer\n",
    "        return layer        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_biases = np.dot(np.ones(self.input.shape[0]), grad_output)\n",
    "        return grad_input, np.r_[grad_weights.ravel(), grad_biases.ravel()]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента\n",
    "\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "relu = ReLU()\n",
    "relu.forward(points)\n",
    "grads = relu.backward(np.ones(points.shape))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: relu.forward(x).sum(), points) \n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "l = Dense(12, 32,)\n",
    "\n",
    "l.forward(x)\n",
    "grads = l.backward(np.ones((10, 32)))[0] \n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(), x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.layer = input - logsumexp(input, axis=1)[:, np.newaxis]\n",
    "        return self.layer\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        res = grad_output + np.exp(self.layer)\n",
    "        return res, []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    return - (activations * target).sum()\n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return - target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "target = np.arange(10)\n",
    "one_hot_targets = np.eye(12)[target]\n",
    "\n",
    "l = Softmax()\n",
    "\n",
    "activations = l.forward(points)\n",
    "grads = l.backward(grad_crossentropy(activations, one_hot_targets))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: crossentropy(l.forward(x), one_hot_targets), x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.eye(10)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797, 10))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка и обучение нейронной сети (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    res = X\n",
    "    for layer in network:\n",
    "        res = layer.forward(res)\n",
    "    return res.argmax(axis=1)   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    In general, the optimization problems are of the form::\n",
      "    \n",
      "        minimize f(x) subject to\n",
      "    \n",
      "        g_i(x) >= 0,  i = 1,...,m\n",
      "        h_j(x)  = 0,  j = 1,...,p\n",
      "    \n",
      "    where x is a vector of one or more variables.\n",
      "    ``g_i(x)`` are the inequality constraints.\n",
      "    ``h_j(x)`` are the equality constrains.\n",
      "    \n",
      "    Optionally, the lower and upper bounds for each element in x can also be\n",
      "    specified using the `bounds` argument.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        Objective function.\n",
      "    x0 : ndarray\n",
      "        Initial guess.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (Jacobian, Hessian).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : bool or callable, optional\n",
      "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the\n",
      "        gradient will be estimated numerically.\n",
      "        `jac` can also be a callable returning the gradient of the\n",
      "        objective. In this case, it must accept the same arguments as `fun`.\n",
      "    hess, hessp : callable, optional\n",
      "        Hessian (matrix of second-order derivatives) of objective function or\n",
      "        Hessian of objective function times an arbitrary vector p.  Only for\n",
      "        Newton-CG, dogleg, trust-ncg.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  If neither `hess` nor\n",
      "        `hessp` is provided, then the Hessian product will be approximated\n",
      "        using finite differences on `jac`. `hessp` must compute the Hessian\n",
      "        times an arbitrary vector.\n",
      "    bounds : sequence, optional\n",
      "        Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
      "        ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the bounds on that parameter. Use None for one of ``min`` or\n",
      "        ``max`` when there is no bound in that direction.\n",
      "    constraints : dict or sequence of dict, optional\n",
      "        Constraints definition (only for COBYLA and SLSQP).\n",
      "        Each constraint is defined in a dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "        current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector.\n",
      "    \n",
      "    **Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    activations = X\n",
    "    for layer in network:\n",
    "        activations = layer.forward(activations)\n",
    "    \n",
    "    loss = crossentropy(activations, y)\n",
    "    gradient = []\n",
    "    grad_out = layer.backward(grad_crossentropy(activations, y))[0]\n",
    "    for layer in network[-2::-1]:\n",
    "        grad_out, grad = layer.backward(grad_out)\n",
    "        gradient = list(grad) + gradient\n",
    "    \n",
    "    return loss, np.array(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00589143,  0.00493999, -0.0081142 , ..., -0.73359351,\n",
       "       -1.79351996, -0.38608186])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_weights(res[\"x\"], network)\n",
    "y_pred_train = predict(network, X_train)\n",
    "y_train_ravel = y_train.argmax(axis=1)\n",
    "(y_train_ravel == y_pred_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = predict(network, X_test)\n",
    "y_test_ravel = y_test.argmax(axis=1)\n",
    "(y_test_ravel == y_pred_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        set_weights(weights, self.network)\n",
    "        y_pred_train = predict(self.network, X_train)\n",
    "        y_train_ravel = y_train.argmax(axis=1)\n",
    "        self.train_acc.append((y_train_ravel == y_pred_train).mean())\n",
    "        \n",
    "        y_pred_test = predict(self.network, X_test)\n",
    "        y_test_ravel = y_test.argmax(axis=1)\n",
    "        self.test_acc.append((y_test_ravel == y_pred_test).mean())\n",
    "        if (print):\n",
    "            print(self.train_acc[-1],'  ', self.test_acc[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10616184112843356    0.08444444444444445\n",
      "0.10467706013363029    0.08222222222222222\n",
      "0.10467706013363029    0.08222222222222222\n",
      "0.20044543429844097    0.18\n",
      "0.20489977728285078    0.19111111111111112\n",
      "0.20638455827765403    0.19555555555555557\n",
      "0.10690423162583519    0.12666666666666668\n",
      "0.18930957683741648    0.21555555555555556\n",
      "0.24944320712694878    0.2822222222222222\n",
      "0.3867854491462509    0.4111111111111111\n",
      "0.5011135857461024    0.5022222222222222\n",
      "0.5723830734966593    0.56\n",
      "0.6451373422420194    0.6044444444444445\n",
      "0.6696362286562731    0.6488888888888888\n",
      "0.6896807720861173    0.6688888888888889\n",
      "0.7030438010393467    0.6688888888888889\n",
      "0.7193763919821826    0.68\n",
      "0.7334818114328137    0.6844444444444444\n",
      "0.7453600593912398    0.7155555555555555\n",
      "0.7757980697847068    0.7466666666666667\n",
      "0.779510022271715    0.7622222222222222\n",
      "0.8054936896807721    0.8044444444444444\n",
      "0.8166295471417966    0.8\n",
      "0.8195991091314031    0.8066666666666666\n",
      "0.8247958426132146    0.8155555555555556\n",
      "0.829992576095026    0.8311111111111111\n",
      "0.8589458054936897    0.8488888888888889\n",
      "0.8559762435040832    0.8288888888888889\n",
      "0.8819599109131403    0.8733333333333333\n",
      "0.8923533778767632    0.88\n",
      "0.8930957683741648    0.8755555555555555\n",
      "0.89086859688196    0.88\n",
      "0.8938381588715665    0.8866666666666667\n",
      "0.8990348923533779    0.9022222222222223\n",
      "0.9012620638455828    0.8977777777777778\n",
      "0.9138827023014106    0.9133333333333333\n",
      "0.9250185597624351    0.92\n",
      "0.9265033407572383    0.9222222222222223\n",
      "0.9302152932442465    0.9266666666666666\n",
      "0.9302152932442465    0.9244444444444444\n",
      "0.9302152932442465    0.9288888888888889\n",
      "0.9406087602078693    0.9288888888888889\n",
      "0.9420935412026726    0.9266666666666666\n",
      "0.9443207126948775    0.92\n",
      "0.949517446176689    0.9266666666666666\n",
      "0.9532293986636972    0.9288888888888889\n",
      "0.9532293986636972    0.9288888888888889\n",
      "0.9569413511507052    0.9288888888888889\n",
      "0.9576837416481069    0.94\n",
      "0.9576837416481069    0.9488888888888889\n",
      "0.9665924276169265    0.9466666666666667\n",
      "0.9673348181143281    0.9422222222222222\n",
      "0.9680772086117297    0.94\n",
      "0.9695619896065331    0.94\n",
      "0.9695619896065331    0.94\n",
      "0.9747587230883444    0.9444444444444444\n",
      "0.9755011135857461    0.9422222222222222\n",
      "0.9784706755753526    0.9466666666666667\n",
      "0.9792130660727543    0.9466666666666667\n",
      "0.9784706755753526    0.9444444444444444\n",
      "0.9784706755753526    0.9422222222222222\n",
      "0.9806978470675576    0.94\n",
      "0.9844097995545658    0.9444444444444444\n",
      "0.985894580549369    0.94\n",
      "0.9866369710467706    0.94\n",
      "0.9888641425389755    0.9422222222222222\n",
      "0.9896065330363771    0.9422222222222222\n",
      "0.9910913140311804    0.94\n",
      "0.9910913140311804    0.9377777777777778\n",
      "0.9910913140311804    0.9355555555555556\n",
      "0.991833704528582    0.9355555555555556\n",
      "0.9896065330363771    0.9355555555555556\n",
      "0.991833704528582    0.94\n",
      "0.9925760950259837    0.9422222222222222\n",
      "0.991833704528582    0.9511111111111111\n",
      "0.9925760950259837    0.9488888888888889\n",
      "0.9933184855233853    0.9488888888888889\n",
      "0.9933184855233853    0.9488888888888889\n",
      "0.994060876020787    0.9422222222222222\n",
      "0.9948032665181886    0.9422222222222222\n",
      "0.9955456570155902    0.9422222222222222\n",
      "0.9962880475129918    0.9488888888888889\n",
      "0.9962880475129918    0.9511111111111111\n",
      "0.9970304380103935    0.9511111111111111\n",
      "0.9970304380103935    0.9488888888888889\n",
      "0.9955456570155902    0.9466666666666667\n",
      "0.9970304380103935    0.9511111111111111\n",
      "0.9970304380103935    0.9511111111111111\n",
      "0.9970304380103935    0.9511111111111111\n",
      "0.9977728285077951    0.9511111111111111\n",
      "0.9977728285077951    0.9466666666666667\n",
      "0.9977728285077951    0.9466666666666667\n",
      "0.9977728285077951    0.9466666666666667\n",
      "0.9985152190051967    0.9466666666666667\n",
      "0.9985152190051967    0.9466666666666667\n",
      "0.9992576095025983    0.9466666666666667\n",
      "0.9992576095025983    0.9466666666666667\n",
      "1.0    0.9533333333333334\n",
      "0.9992576095025983    0.9466666666666667\n",
      "0.9992576095025983    0.9511111111111111\n",
      "0.9992576095025983    0.9511111111111111\n",
      "1.0    0.9533333333333334\n",
      "1.0    0.9466666666666667\n",
      "0.9992576095025983    0.9488888888888889\n",
      "1.0    0.9466666666666667\n",
      "0.9992576095025983    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9511111111111111\n",
      "1.0    0.9511111111111111\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9422222222222222\n",
      "1.0    0.9422222222222222\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9422222222222222\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9377777777777778\n",
      "1.0    0.9377777777777778\n",
      "1.0    0.94\n",
      "1.0    0.9377777777777778\n",
      "1.0    0.9355555555555556\n",
      "1.0    0.94\n",
      "1.0    0.94\n",
      "1.0    0.94\n",
      "1.0    0.9355555555555556\n",
      "1.0    0.9377777777777778\n",
      "1.0    0.9377777777777778\n",
      "1.0    0.94\n",
      "1.0    0.94\n",
      "1.0    0.94\n",
      "1.0    0.94\n",
      "1.0    0.9422222222222222\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9444444444444444\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9511111111111111\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9511111111111111\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9488888888888889\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n",
      "1.0    0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAEbCAYAAAAS12RiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDOryhyKJrrYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f228116cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(9)\n",
    "fig.set_figheight(4)\n",
    "\n",
    "plt.plot(cb.train_acc, label=\"train acc\", linewidth=3)\n",
    "plt.plot(cb.test_acc, label=\"test acc\", linewidth=3)\n",
    "plt.xlabel(\"Iteration\", fontsize=13)\n",
    "plt.ylabel(\"Accuracy\", fontsize=13)\n",
    "plt.title(\"Зависимость качества от числа итераций\", fontsize=15)\n",
    "plt.xlim(0, len(cb.train_acc))\n",
    "plt.ylim(0.4, 1.01)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=4, fontsize='large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с числом слоев (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layers_size = 32\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        network = []\n",
    "        if i == 0:\n",
    "            network.append(Dense(X_train.shape[1], 10))\n",
    "        else:\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            for k in range(0, i - 1):\n",
    "                network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "                network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "        network.append(Softmax())\n",
    "        weights = get_weights(network)\n",
    "        res = minimize(compute_loss_grad, weights, args=[network, X_train, y_train],\n",
    "               method=\"L-BFGS-B\", jac=True)\n",
    "        set_weights(res[\"x\"], network)\n",
    "        y_pred_train = predict(network, X_train)\n",
    "        y_train_ravel = y_train.argmax(axis=1)\n",
    "        accs_train[i, j] = (y_train_ravel == y_pred_train).mean()\n",
    "        y_pred_test = predict(network, X_test)\n",
    "        y_test_ravel = y_test.argmax(axis=1)\n",
    "        accs_test[i, j] = (y_test_ravel == y_pred_test).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.10616184, 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94222222, 0.94888889, 0.95111111, 0.94222222, 0.94666667],\n",
       "       [0.96222222, 0.95555556, 0.95111111, 0.95777778, 0.95777778],\n",
       "       [0.95777778, 0.96      , 0.95111111, 0.95777778, 0.93555556],\n",
       "       [0.92666667, 0.95555556, 0.95555556, 0.95777778, 0.95333333],\n",
       "       [0.08444444, 0.93333333, 0.94222222, 0.94222222, 0.94444444]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test quality in 5 runs')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f228133f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "\n",
    "При увеличении числа слоев до 3 качество обучения растет, затем с увеличением слоев начинает падать. Вместе с увеличением числа слоев также немного увеличивается и разброс.\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "\n",
    "Логистическая регрессия соотвествует в данном случае нейросети с одним полносвязным слоем. По графикам видно, что линейная модель дает меньшее качество, чем нейросеть с 2-4 слоями, но лучше чем при 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация метода оптимизации (1 балл)\n",
    "\n",
    "Реализуйте сами метод оптимизации (аналог функции minimize) для рассмотренной выше архитектуры. В качестве метода оптимизации используйте SGD + momentum. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с LBFGS-B. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout (1 балл) \n",
    "\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр p). Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization (1 балл)\n",
    "\n",
    "Реализуйте слой BatchNormalization. Сравните обучение сети из большого числа слоёв при использовании BatchNormalization и без его использования.  Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
